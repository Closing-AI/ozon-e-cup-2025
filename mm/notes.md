# multimodal

сценарии:

- простой late-fusion: нормализуем каждый вектор (текст, табличные, картинка), замем конкатим. обучаем классификаторы (логрег, бустинг, млп)

  - norm + concat + модели из sklearn
  - norm + concat + xgboost
  - LayerNorm + concat + torch NN

- добавить self-attention перед моделью

  - LayerNorm + concat + att + mlp
  - LayerNorm + concat + att + boosting (тут вначале учим модель выше, потом убираем mlp и делаем бустинг)

- modality gating + attention (с этим предлагаю пока повременить)

использовать:

- modality dropout - бинарные фичи (есть/нет картинка/текст/табличные), нули для отсутствующих (также можно самим занулять что-то для устойчивости модели)
- нормализация - layernorm, standartization
- optuna для подбора гиперпараметров

подумать:

- проекция эмбедов в 1 пространство (clip-like)
- разделители при формировании seq для attn

нормализация:

- табличные/числовые фичи - standard scaling
- эмбеддинги текст/изображение - l2 norm per sample
- бинарные фичи - ничего не делаем

attention:

- делаем self-att
- тк attn блок принимает вход следующей формы (batch_size, seq_len, emb_dim), то мы можем сделать так:
  
  - легковесный способ: сформировать векторы из каждой модальности (текст, изображение, метаданные, булевы флаги) - т.е получится н-р 5 векторов -> mlp для приведение размерностей в 1 -> получаем на вход тензор (batch_size, 5, emb_dim).
  - более затратный: эмбеддинги текста и изображения вытащить по-токенные -> будет следующая последовательность векторов на вход: (вектор (1) метаданных, булевых, текстовых фичей; разделитель; потокенные векторы изображения; разделитель; потокенные векторы текста). Конечно все вектора приведем в 1 размерность.
